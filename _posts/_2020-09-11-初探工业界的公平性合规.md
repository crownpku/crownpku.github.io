---
layout: post
comments: true
title: 初探工业界的公平性合规
published: true
---

你在铁轨上散步，一辆火车轰隆隆驶来，前面有玩耍的孩子；如果你无动于衷，火车会撞到前面十几个个孩子，如果你扳动身边的扳手，火车会换轨撞到另一侧的三四个孩子。你会怎么做？

这是哈佛大学 [Justice](https://online-learning.harvard.edu/course/justice?delta=1) 课程第一节课上教授抛给学生的问题。

我们已经聊过几次从机器学习模型角度出发的公平正义性的问题，如：

* [Algorithms Auditing：你的代码公平正义吗？](http://www.crownpku.com/2018/11/14/Algorithms-Auditing-%E4%BD%A0%E7%9A%84%E4%BB%A3%E7%A0%81%E5%85%AC%E5%B9%B3%E6%AD%A3%E4%B9%89%E5%90%97.html)
* [机器学习模型的公平性评测](http://www.crownpku.com/2020/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%AC%E5%B9%B3%E6%80%A7%E8%AF%84%E6%B5%8B.html)
* [GPT-3 的算法偏见](http://www.crownpku.com/2020/09/03/%E4%B8%8EGPT-3%E8%81%8A%E5%A4%A9%E6%90%9E%E5%AE%9Achatbot%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE.html)

工业界乃至哲学界对于公平性的讨论早在人工智能出现前很久就已经存在。从机器学习算法角度出发的模型公平性方案距离真正解决工业界的合规要求还有多远？我们暂时摘掉技术这顶帽子，从工业界的角度针对公平性合规这件事窥豹一斑。

本文开头提到的例子并没有正确答案，每个人有自己的理解和决定，也反映出公平和正义灰色的一面。实际中工业界关于法规关于公平性的定义也并没有统一的标准。下图是澳大利亚不同州对各种特定因素的反歧视法律规定：

![](/images/202009/2.png)

我们看到单单是同一个国家中的不同州，对一些特定因素（敏感特征）上的法规已经有所不同，我们可以想象这是一个多么复杂的问题。上面这幅图引入了受保护特征的概念，在这些规定的受保护特征上面如果有歧视和不公平的发生就可能触犯法律。

不只在澳大利亚，美国的 ECOA 法规要求对银行借贷被拒绝的情况为贷款人提供拒贷原因，信贷评估过程要求有历史经验指导、模型可解释性和数据统计学支持。一个典型的嵌有信贷模型的银行信贷评估过程如下图：

![](/images/202009/3.png)

我们看到在模型给出风险分数之后，还会通过很多的补充规则进行过滤，才会最终给出审批结果。这部分规则一方面限制了模型结果中的越界噪音，一方面在如经济情势突然转差或意外事件发生时可以敏捷地修改规则保证风险可控，同时一些规则也可以对模型结果的公平性合规性进行一定程度的补充优化。

实际情况中不同银行会有不同的规则、模型和数据储备。笔者刚到新加坡时用第一个月的工资单申请了 A 和 B 两家银行的信用卡，结果 A 以历史记录不足为由直接悲剧，而 B 在追要到笔者之前香港的工资情况后顺利获批，A 与 B 在保证自身信贷风险和公平合规的前提下，谁的信贷业务更进取更灵活就非常明显了。

欧洲的 GDPR 法规更进一步，要求所有自动化决策系统提供决策背后可解释的逻辑，并要给相关人士提供质疑决策结果和要求人工介入复查的机会。如果新加坡有类似的法规，估计我会去信 A 公司小小投诉一下。

新加坡HDB种族融合，美国postcode隐含种族信息。

白人（绿色）拉丁裔（黄色）非洲裔（蓝色）亚裔（红色）

![](/images/202009/4.png)





## Reference
https://actuaries.asn.au/Library/Miscellaneous/2020/ADWGPaperFinal.pdf



