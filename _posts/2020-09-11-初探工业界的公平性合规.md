---
layout: post
comments: true
title: 初探工业界的公平性合规
published: true
---

你在铁轨上散步，一辆火车轰隆隆驶来，前面有玩耍的孩子；如果你无动于衷，火车会撞到前面十几个个孩子，如果你扳动身边的扳手，火车会换轨撞到另一侧的三四个孩子。你会怎么做？

这是哈佛大学 [Justice](https://online-learning.harvard.edu/course/justice?delta=1) 课程第一节课上教授抛给学生的问题。

我们已经聊过几次从机器学习模型角度出发的公平正义性的问题，如：

* [Algorithms Auditing：你的代码公平正义吗？](http://www.crownpku.com/2018/11/14/Algorithms-Auditing-%E4%BD%A0%E7%9A%84%E4%BB%A3%E7%A0%81%E5%85%AC%E5%B9%B3%E6%AD%A3%E4%B9%89%E5%90%97.html)
* [机器学习模型的公平性评测](http://www.crownpku.com/2020/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%AC%E5%B9%B3%E6%80%A7%E8%AF%84%E6%B5%8B.html)
* [GPT-3 的算法偏见](http://www.crownpku.com/2020/09/03/%E4%B8%8EGPT-3%E8%81%8A%E5%A4%A9%E6%90%9E%E5%AE%9Achatbot%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE.html)

工业界乃至哲学界对于公平性的讨论早在人工智能出现前很久就已经存在。从机器学习算法角度出发的模型公平性方案距离真正解决工业界的合规要求还有多远？我们暂时摘下人工智能这顶帽子，从工业界的角度针对公平性合规这件事窥豹一斑。

本文开头提到的例子并没有正确答案，每个人有自己的理解和决定，也反映出公平和正义灰色的一面。实际中工业界关于法规关于公平性的定义也并没有统一的标准。下图是澳大利亚不同州对各种特定因素的反歧视法律规定：

![](/images/202009/2.png)

我们看到单单是同一个国家中的不同州，对一些特定因素（敏感特征）上的法规已经有所不同，可以想象这是一个多么复杂的问题。上面这幅图引入了受保护特征的概念，在这些规定的受保护特征上面如果有歧视和不公平的发生就可能触犯法律。

歧视和不公平简单可以分为几种不同的类别，如机会的不公平（某类人的升学机会远大于另一些人），服务质量的不公平（某类人的人脸识别效果远差过另一些人）等。

不只在澳大利亚，美国的 ECOA 法规要求对银行借贷被拒绝的情况为贷款人提供拒贷原因，信贷评估过程要求有历史经验指导、模型可解释性和数据统计学支持。一个典型的嵌有信贷模型的银行信贷评估过程如下图：

![](/images/202009/3.png)

我们看到在模型给出风险分数之后，还会通过很多的补充规则进行过滤，才会最终给出审批结果。这部分规则一方面限制了模型结果中的越界噪音，一方面在如经济情势突然转差或意外事件发生时可以快速修改规则保证风险可控，同时一些规则也可以对模型结果的公平性合规性进行一定程度的补充优化。

实际情况中不同银行会有不同的规则、模型和数据储备。笔者刚到新加坡时用第一个月的工资单申请了 A 和 B 两家银行的信用卡，结果 A 以历史记录不足为由直接杯具，而 B 在追要到笔者之前香港的工资情况后顺利获批，A 与 B 在保证自身信贷风险和公平合规的前提下，谁的信贷业务更先进更灵活就非常明显了。

欧洲的 GDPR 法规更进一步，要求所有自动化决策系统提供决策背后可解释的逻辑，并要给相关人士提供质疑决策结果和要求人工介入复查的机会。如果你被拒贷，银行需要提供你被拒绝的理由，甚至可以提供一些建议，比如提高收入或者减少贷款额度可能会增加你拿到贷款的机会。

对一些敏感特征如种族，不同国家的情形和措施也差别很大。新加坡的社区要求种族融合，每个社区尤其是政府组屋需要有华裔、马来裔、印度裔等不同种族比例的住户，且新加坡总统也由不同种族轮流担任。美国的社区种族分离则比较严重，邮政编码的信息已经往往隐含了一些种族信息。

下图是美国纽约的种群聚居分布地图，白人是绿色，拉丁裔是黄色，非洲裔是蓝色，亚裔是红色。

![](/images/202009/4.png)

这也是为什么我们不建议在模型训练过程中直接删除某个敏感信息，因为很多其它维度的特征可能仍然可以推导出用户的敏感信息。上面的例子中，如果某个模型不能使用种族作为评判标准，那么居住地址的邮政编码很大机会也不应该被允许。

但类似的特征耦合对于公平性合规的影响又并不是绝对的。在公平和反歧视法例之下会有很多实际的执行问题，也需要实际中灵活性的评判。

以车险为例，在欧洲车险是不能有性别歧视的，保险公司不能因为是男司机或者女司机而收取更高的保费。但是车险保费一般和引擎排量成正比的，大排量的汽车会被收取更高的保费。而相比女司机，男司机更倾向于购买大排量的汽车，所以结果就是男司机的平均保费会比女司机高很多。

欧洲法例认为这是合规的。虽然性别歧视不被允许，但引擎排量可以用作保费评判标准，因为引擎排量确实是非常强的风险提示因子（大排量车往往是豪车，撞一下要赔很多钱）。男司机也完全可以选择购买小排量汽车来减少保费，这是个人自由选择的问题。

类似这样的反歧视法例，在保险业实际应用中还有很多可以豁免的例子。人寿险中女士的保费往往要比男士高，因为大量数据表明女人的平均寿命要比男人高，在精算标准中理应收取更高保费，因此在性别歧视法例上可以豁免。

总体来讲，保险业关于公平性一般有以下两个原则：

1. 对用户保险产品的核保是合规的，如果该核保决策是基于精算或统计学的数据支持，且其中的决策差异化基于数据和其它相关因素是合理的（有数据合规）
2. 对用户保险产品的核保是合规的，如果该核保决策相关的精算或统计学的数据并不存在或不能获取，且其中的决策差异化基于其它相关因素是合理的（无数据合规）

上面的原则就保证了无论是基于传统精算还是基于人工智能的保险核保决策系统，对于敏感特征的公平性要求有一定的豁免权，但这一定是建立在有充分数据支持和合理解释的前提下，且要保留证据受监管者审核。

虽然保险并不是公益，保险公司有自己经济利益的多重考量，但保险对于整个社会稳定有重要意义，所以保险成为一个强监管的行业。大部分的情况下，就算核保决策系统对投保人的风险提出警示，保险公司依然不会直接拒绝投保人，而会使用如增加保费或增加投保人历史疾病的相关理赔免责条款的方式给出保单，在保证保险依然可覆盖最大范围人群的基础上控制财务风险。

当然保险公司现在也越来越像一个基于数据的健康管理公司，除了传统保险服务以外，也会在 app 里提醒你多运动，免费提供体检等等，因为你自己越健康，保险公司的风险也就越低，大家共赢。

工业界的公平性和反歧视合规依然是一个发展中的复杂话题，尤其在数据和算法越来越掌控着决策系统的今天。如何制定合理的公平性法规，如何在实际应用中严格又灵活地实践公平性准则，如何在系统中实现和嵌入对公平性的检测与改进，都值得我们继续不断探讨、谨慎实践。

## Reference
https://actuaries.asn.au/Library/Miscellaneous/2020/ADWGPaperFinal.pdf



