---
layout: post
comments: true
title: 用Shapley值解释联邦学习模型
published: true
---

### 代码在[https://github.com/crownpku/federated_shap](https://github.com/crownpku/federated_shap)

### 论文在[https://arxiv.org/abs/1905.04519](https://arxiv.org/abs/1905.04519)


目标会议截稿日期的前四天，在讨论工作项目的时候有了些许灵感，决定着手计划尝试写些实验代码。闭关了一共四天，每天早上早起去海边跑步，理清思路，列好计划，回来冲个凉水澡就开始搬砖到晚上。在截稿日期的前一晚做完所有的工作，提交了论文，合上电脑，长舒一口气；这么久没写过论文，上一次这样的状态可能还是在八年前的港科大图书馆。

后来发现会议的截稿日期在最后时刻延长了。果真惊叹deadline是第一生产力。如果早早知道截稿会延期，可能就没了那些动力闭关做事情，最后也许就像很多自己心血来潮要做的事情一样不了了之了吧。

当然急急赶出来的论文一定有很多疏漏和问题，也请各位多多指教。

## 用Shapley值来解释机器学习模型

我们知道机器学习模型很多都是黑盒或者半黑盒模型。如果模型是用来做语音识别、图像识别之类的感知任务，可能人们就不会太过关心模型的可解释性问题；当然也有很多对抗学习的工作在研究感知模型，想必很多人对那个熊猫加没信号的电视机就变成长臂猿的例子记忆犹新，但这些工作可能更多是从模型可靠性的角度出发。

![]()

对于另一些关键应用中的非感知类的机器学习模型，比如贷款风险估算，保险核保预测，欺诈识别模型等等，模型的可解释性就尤为重要。我在之前的一篇博客[Algorithms Auditing：你的代码公平正义吗？](http://www.crownpku.com/2018/11/14/Algorithms-Auditing-%E4%BD%A0%E7%9A%84%E4%BB%A3%E7%A0%81%E5%85%AC%E5%B9%B3%E6%AD%A3%E4%B9%89%E5%90%97.html)里面有大致聊过一些我认为的机器学习模型审计重要性和一些方法。其中一个很有意思的跟模型算法独立的模型解释方法就是用Shapley值来计算模型特征的重要性，也是[SHAP](https://github.com/slundberg/shap)这个工具包背后的重要思想。SHAP以我了解已经被很多做金融和保险机器学习模型的同僚们应用在实际工作中了，瓶颈可能不在技术而更多在于如何给合规与法律部门的同事解释清楚这个东西到底是怎么回事...

我们假设已经训练好了一个模型f(x)，把特征向量x映射去一个预测值y，这里x包含了n个不同的特征，x={x1, x2, ..., xn}。我们想知道每一个特征在这个模型中的起到了怎样的作用。这包含了两个层面：我们可以对每一个单独的预测感兴趣，用特征重要程度来试图解释每一个特定的决策；我们也可以对整个模型感兴趣，想知道每个特征在模型做出大量决策时的重要度。对于后者，也就是整个模型的特征重要性，其实如xgboost这样的工具包已经包含了计算的功能。SHAP更多是聚焦于前者。实际应用中对单个决策的解释也是非常重要，因为人们会问凭什么你给李雷放了贷款而不给我放，凭什么韩梅梅的保费就是比我低etc.这样的问题。

Shapley值源自于博弈论，应用在模型解释性的思路也是很直接了当。模型f已经有了，对于一个特定决策，所有特征{x1, x2, ..., xn}的具体值也在。我们把每个特征当作一个开关，尝试构造一系列模拟的特征值，每一次都把{x1, x2, ..., xn}中的一些特征“开启”和“关掉”，然后计算预测结果和原始结果的偏差。当我们尝试尽所有可能的特征“开启”和“关掉”的组合，就有了一系列这样的偏差值。我们现在要计算特征xi的重要性，只要参考除了xi之外的其它所有特征排列组合下，xi关掉和开启的两种情况对原始结果造成的平均波动就好了。

这样的思路，就是完全把模型当作一个黑盒子，不管你的盒子是神经网络做的还是随机森林做的，不断尝试拔掉和接上炸弹上面有颜色的电线，观察爆炸的猛烈程度来判断哪个电线是重要的哪个是没用的...

## 联邦学习



